{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5df77fad",
      "metadata": {
        "id": "5df77fad"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "#nltk.download()\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from keras.layers import Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdd51a38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdd51a38",
        "outputId": "398590f5-b5de-48b7-f85e-351f437cc7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'imgflipcom', 'zip']\n"
          ]
        }
      ],
      "source": [
        "data1=pd.read_csv('training.csv', sep='\\t')\n",
        "data2=pd.read_csv('trial.csv', sep='\\t')\n",
        "y=data1['misogynous']\n",
        "# print(training_data.head())\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "stop_words=stopwords.words('english')\n",
        "stop_words.append('imgflipcom')\n",
        "stop_words.append('zip')\n",
        "print(stop_words)\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "#training data\n",
        "for index,row in data1.iterrows():\n",
        "  #print(row)\n",
        "  filter_sentence=[]\n",
        "  sentence=row['Text Transcription']\n",
        "  sentence = sentence.lower()\n",
        "  #print(sentence)\n",
        "  sentence=re.sub(r'[^\\w\\s]','',sentence)#cleaning\n",
        "  words=nltk.word_tokenize(sentence)\n",
        "  words=[w for w in words if not w in stop_words]\n",
        "  for word in words:\n",
        "    filter_sentence.append(lemmatizer.lemmatize(word))\n",
        "  #print(filter_sentence)\n",
        "  listToStr = ' '.join([str(elem) for elem in filter_sentence])\n",
        "  data1.loc[index,\"Text Transcription\"]=listToStr\n",
        "#trail data\n",
        "for index,row in data2.iterrows():\n",
        "  #print(row)\n",
        "    filter_sentence=[]\n",
        "    sentence=row['Text Transcription']\n",
        "    sentence = sentence.lower()\n",
        "    #print(sentence)\n",
        "    sentence=re.sub(r'[^\\w\\s]','',sentence)#cleaning\n",
        "    words=nltk.word_tokenize(sentence)\n",
        "    words=[w for w in words if not w in stop_words]\n",
        "    for word in words:\n",
        "        filter_sentence.append(lemmatizer.lemmatize(word))\n",
        "    #print(filter_sentence)\n",
        "    listToStr = ' '.join([str(elem) for elem in filter_sentence])\n",
        "    data2.loc[index,\"Text Transcription\"]=listToStr\n",
        "#print(data1['Text Transcription'].head())\n",
        "#print(data1.shape)\n",
        "#print(data2.head())\n",
        "#print(data2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")"
      ],
      "metadata": {
        "id": "Px8pIrn3PAiE"
      },
      "id": "Px8pIrn3PAiE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "125825c8",
      "metadata": {
        "id": "125825c8"
      },
      "outputs": [],
      "source": [
        "def get_max_length(df):\n",
        "    \n",
        "#get max token counts from train data, so we use this number as fixed length input to LSTM cell\n",
        "    \n",
        "    max_length = 0\n",
        "    for row in df['Text Transcription']:\n",
        "        if len(row.split(\" \")) > max_length:\n",
        "            max_length = len(row.split(\" \"))\n",
        "    return max_length\n",
        "\n",
        "def word2vec(reviews):\n",
        "#get word2vec value for each word in sentence.concatenate word in numpy array, so we can use it as LSTM input\n",
        "    encoded_reviews = []\n",
        "    for review in reviews:\n",
        "        tokens = review.split(\" \")\n",
        "        word2vec_embedding = embed(tokens)\n",
        "        encoded_reviews.append(word2vec_embedding)\n",
        "    return encoded_reviews\n",
        "        \n",
        "def padded_encoded(encoded_reviews):\n",
        "#for short sentences, we prepend zero padding so all input to LSTM has same length\n",
        "    padded_reviews_encoding = []\n",
        "    for enc_review in encoded_reviews:\n",
        "        zero_padding_cnt = max_length - enc_review.shape[0]\n",
        "        pad = np.zeros((1, 250))\n",
        "        for i in range(zero_padding_cnt):\n",
        "            enc_review = np.concatenate((pad, enc_review), axis=0)\n",
        "        padded_reviews_encoding.append(enc_review)\n",
        "    return padded_reviews_encoding\n",
        "\n",
        "def sentiment_encode(sentiment):\n",
        "#return one hot encoding for Y value\n",
        "    if sentiment == 1:\n",
        "        return [1,0]\n",
        "    else:\n",
        "        return [0,1]\n",
        "    \n",
        "def preprocess(df):\n",
        "#encode text value to numeric value\n",
        "# encode words into word2vec\n",
        "    reviews = df['Text Transcription'].tolist()\n",
        "    \n",
        "    encoded_reviews = word2vec(reviews)\n",
        "    padded_encoded_reviews = padded_encoded(encoded_reviews)\n",
        "    # encoded sentiment\n",
        "    sentiments = df['misogynous'].tolist()\n",
        "    encoded_sentiment = [sentiment_encode(sentiment) for sentiment in sentiments]\n",
        "    X = np.array(padded_encoded_reviews)\n",
        "    Y = np.array(encoded_sentiment)\n",
        "    return X, Y "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = get_max_length(data1)\n",
        "\n",
        "train_X, train_Y = preprocess(data1)"
      ],
      "metadata": {
        "id": "ueaCJ9sLNwrQ"
      },
      "id": "ueaCJ9sLNwrQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model=Sequential()\n",
        "model.add((LSTM(64)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(2,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "tpWpjvJMNw3p"
      },
      "id": "tpWpjvJMNw3p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_X,train_Y,validation_split=0.2,epochs=20,batch_size=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_Frr88CQgNk",
        "outputId": "360c3a3e-cafb-4242-d7ad-9a1562e8b675"
      },
      "id": "M_Frr88CQgNk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "80/80 [==============================] - 26s 294ms/step - loss: 0.6721 - accuracy: 0.5969 - val_loss: 0.6524 - val_accuracy: 0.6205\n",
            "Epoch 2/20\n",
            "80/80 [==============================] - 23s 290ms/step - loss: 0.6327 - accuracy: 0.6511 - val_loss: 0.6316 - val_accuracy: 0.6580\n",
            "Epoch 3/20\n",
            "80/80 [==============================] - 23s 291ms/step - loss: 0.6047 - accuracy: 0.6744 - val_loss: 0.6116 - val_accuracy: 0.6680\n",
            "Epoch 4/20\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.5892 - accuracy: 0.6880 - val_loss: 0.6043 - val_accuracy: 0.6745\n",
            "Epoch 5/20\n",
            "80/80 [==============================] - 23s 288ms/step - loss: 0.5721 - accuracy: 0.6995 - val_loss: 0.5994 - val_accuracy: 0.6825\n",
            "Epoch 6/20\n",
            "80/80 [==============================] - 23s 288ms/step - loss: 0.5616 - accuracy: 0.7078 - val_loss: 0.5997 - val_accuracy: 0.6840\n",
            "Epoch 7/20\n",
            "80/80 [==============================] - 23s 290ms/step - loss: 0.5501 - accuracy: 0.7190 - val_loss: 0.6001 - val_accuracy: 0.6800\n",
            "Epoch 8/20\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.5371 - accuracy: 0.7330 - val_loss: 0.5886 - val_accuracy: 0.6980\n",
            "Epoch 9/20\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.5306 - accuracy: 0.7352 - val_loss: 0.5960 - val_accuracy: 0.6945\n",
            "Epoch 10/20\n",
            "80/80 [==============================] - 24s 295ms/step - loss: 0.5284 - accuracy: 0.7374 - val_loss: 0.5762 - val_accuracy: 0.6990\n",
            "Epoch 11/20\n",
            "80/80 [==============================] - 23s 285ms/step - loss: 0.5142 - accuracy: 0.7475 - val_loss: 0.5949 - val_accuracy: 0.6875\n",
            "Epoch 12/20\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.5057 - accuracy: 0.7481 - val_loss: 0.5867 - val_accuracy: 0.7015\n",
            "Epoch 13/20\n",
            "80/80 [==============================] - 24s 295ms/step - loss: 0.5009 - accuracy: 0.7533 - val_loss: 0.5847 - val_accuracy: 0.7095\n",
            "Epoch 14/20\n",
            "80/80 [==============================] - 23s 289ms/step - loss: 0.4918 - accuracy: 0.7616 - val_loss: 0.5823 - val_accuracy: 0.7020\n",
            "Epoch 15/20\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.4845 - accuracy: 0.7611 - val_loss: 0.5662 - val_accuracy: 0.7170\n",
            "Epoch 16/20\n",
            "80/80 [==============================] - 23s 288ms/step - loss: 0.4760 - accuracy: 0.7678 - val_loss: 0.5762 - val_accuracy: 0.7135\n",
            "Epoch 17/20\n",
            "80/80 [==============================] - 23s 289ms/step - loss: 0.4712 - accuracy: 0.7689 - val_loss: 0.5993 - val_accuracy: 0.7010\n",
            "Epoch 18/20\n",
            "80/80 [==============================] - 24s 295ms/step - loss: 0.4674 - accuracy: 0.7694 - val_loss: 0.5710 - val_accuracy: 0.7045\n",
            "Epoch 19/20\n",
            "80/80 [==============================] - 22s 281ms/step - loss: 0.4600 - accuracy: 0.7818 - val_loss: 0.5660 - val_accuracy: 0.7210\n",
            "Epoch 20/20\n",
            "80/80 [==============================] - 22s 280ms/step - loss: 0.4508 - accuracy: 0.7836 - val_loss: 0.5879 - val_accuracy: 0.7150\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f33f6d1be50>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_X, test_Y = preprocess(data2)"
      ],
      "metadata": {
        "id": "OGDHVoADQxXS"
      },
      "id": "OGDHVoADQxXS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_x=model.predict(test_X) \n",
        "y_pred=np.argmax(predict_x,axis=1)"
      ],
      "metadata": {
        "id": "-lUbYR8PQxbm"
      },
      "id": "-lUbYR8PQxbm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(test_X, test_Y, verbose=2)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52Fm5w_HRlHe",
        "outputId": "57f84779-05d9-4774-9093-16ce5b26a9e4"
      },
      "id": "52Fm5w_HRlHe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 - 0s - loss: 0.4617 - accuracy: 0.8000 - 125ms/epoch - 31ms/step\n",
            "Test score: 0.46173787117004395\n",
            "Test accuracy: 0.800000011920929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fskL9Xn9VnCz"
      },
      "id": "fskL9Xn9VnCz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "LSTM_TEXT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}